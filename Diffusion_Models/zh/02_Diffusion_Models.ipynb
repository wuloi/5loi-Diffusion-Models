{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0dyhr7k_7DS"
   },
   "source": [
    "<center><a href=\"https://5loi.com/about_loi\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBiybVKy_2zi"
   },
   "source": [
    "# 2. Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtLP1aneFBZp"
   },
   "source": [
    "In the previous notebook, we learned how to separate noise from an image using a U-Net, but it was not capable of generating believable new images from noise. Diffusion models are much better at generating images from scratch.\n",
    "\n",
    "The good news, our neural network model will not change much. We will be building off of the U-Net architecture with some slight modifications.\n",
    "\n",
    "Instead, the big difference is how we use our model. Rather than adding noise to our images all at once, we will be adding a small amount of noise multiple times. We can then use our neural network on a noisy image multiple times to generate a new image like so:\n",
    "\n",
    "<center><img src=\"images/rev_diffusion.png\" /></center>\n",
    "\n",
    "#### Learning Objectives\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Construct a forward diffusion variance schedule\n",
    "* Define the forward diffusion function, `q`\n",
    "* Update the U-Net architecture to accommodate a timestep, `t`\n",
    "* Train a model to detect noise added to an image based on the timestep `t`\n",
    "* Define a reverse diffusion function to emulate `p`\n",
    "* Attempt to generate articles of clothing (again)\n",
    "\n",
    "We've moved some of the functions from the previous notebook into a [utils.py](utils/utils.py) file. We can use it to reload the fashionMNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "# User defined libraries\n",
    "from utils import other_utils\n",
    "\n",
    "IMG_SIZE = 16\n",
    "IMG_CH = 1\n",
    "BATCH_SIZE = 128\n",
    "data, dataloader = other_utils.load_transformed_fashionMNIST(IMG_SIZE, BATCH_SIZE)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Forward Diffusion\n",
    "\n",
    "Let `T` be the number of times we will add noise to an image. We can use `t` to keep track of the current `timestep`.\n",
    "\n",
    "In the previous notebook, we used the term `beta` to represent what percentage of the new image was noise compared to the original image. The default was 50% noise and 50% the original image. This time, we will use a `variance schedule`, represented as $\\beta_t$, or `B` in code. This will describe how much noise will be added to our image at each timestep `t`.\n",
    "\n",
    "In section 4 of the paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239?ref=assemblyai.com), the authors discuss the art of defining a good schedule. It should be large enough for the model to recognize noise was added (especially since the image may already be noisy), but still as small as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-k7itgbnwZ4y"
   },
   "outputs": [],
   "source": [
    "nrows = 10\n",
    "ncols = 15\n",
    "\n",
    "T = nrows * ncols\n",
    "start = 0.0001\n",
    "end = 0.02\n",
    "B = torch.linspace(start, end, T).to(device)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zx7ajI-1Q1T_"
   },
   "source": [
    "A [Normal Dsitribution](https://mathworld.wolfram.com/NormalDistribution.html) has the following signature:\n",
    "\n",
    "$\\mathcal{N}(x;u,\\sigma^2)$ = $\\frac{1}{\\sigma\\sqrt{2\\pi}}\\mathcal{e}^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}$\n",
    "\n",
    "which reads as, \"the normal distribution of $x$ with parameters $u$ (the mean) and $\\sigma^2$ (the variance). When $\\mu$ is 0 amd $\\sigma$ is 1, we have a standard normal distribution $\\mathcal{N}(x;0,1)$, which has the probability density of the shape below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/normal.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are altering our image with noise multiple times accross many timesteps, let's describe $\\mathbf{x}_{t}$ as our image at timestep $t$. Then, $\\mathbf{x}_{t-1}$ would be the image at the previous timestep and $x_{0}$ would be the original image.\n",
    "\n",
    "In the previous notebook, we added noise to images using the equation:\n",
    "\n",
    "$q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})=\\mathcal{N}(\\mathbf{x}_{t};(1-\\beta_{t}) \\cdot \\mathbf{x}_{t-1},\\beta_{t}^{2}  \\cdot \\mathbf{I})$\n",
    "\n",
    "Where $q$ represents a probability distribution for the [forward diffusion process](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#forward-diffusion-process) and $q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})$ describes the probability distribution for a new, noiser image $\\mathbf{x}_{t}$ based on $\\mathbf{x}_{t-1}$.\n",
    "\n",
    "This time, we will alter images with the similar equation:\n",
    "\n",
    "$q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_{t}} \\cdot \\mathbf{x}_{t-1},\\beta_{t} \\cdot \\mathbf{I})$\n",
    "\n",
    "We can sample from this probability distribution by first sampling from a standard normal distribution $\\mathcal{N}(x;0,1)$ using [torch.randn_like](https://pytorch.org/docs/stable/generated/torch.randn_like.html):\n",
    "\n",
    "`noise = torch.randn_like(x_t)`\n",
    "\n",
    "We can then multiply and add to the noise to sample from `q`:\n",
    "\n",
    "`x_t = torch.sqrt(1 - B[t]) * x_t + torch.sqrt(B[t]) * noise`\n",
    "\n",
    "Let's see all of this in practice. Run the code cell below to perform forward diffusion `T` (or `150`) times on the first image of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "executionInfo": {
     "elapsed": 41034,
     "status": "ok",
     "timestamp": 1690448390469,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "UzIMeK6YRmIt",
    "outputId": "5c90fddf-43bc-4f3f-8374-6a4609bdd79e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "x_0 = data[0][0].to(device)  # Initial image\n",
    "x_t = x_0  # Set up recursion\n",
    "xs = []  # Store x_t for each T to see change\n",
    "\n",
    "for t in range(T):\n",
    "    noise = torch.randn_like(x_t)\n",
    "    x_t = torch.sqrt(1 - B[t]) * x_t + torch.sqrt(B[t]) * noise  # sample from q(x_t|x_t-1)\n",
    "    img = torch.squeeze(x_t).cpu()\n",
    "    xs.append(img)\n",
    "    ax = plt.subplot(nrows, ncols, t + 1)\n",
    "    ax.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "plt.savefig(\"forward_diffusion.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72jh9eF33R12"
   },
   "source": [
    "Or in animated form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 910,
     "output_embedded_package_id": "1RQgJ25OqX2eHWritA1w734Vn0JzygjtQ"
    },
    "executionInfo": {
     "elapsed": 18548,
     "status": "ok",
     "timestamp": 1690448409015,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "TMexBq_Px8si",
    "outputId": "6b34799c-d21c-4955-ca08-702524296bdd"
   },
   "outputs": [],
   "source": [
    "gif_name = \"forward_diffusion.gif\"\n",
    "other_utils.save_animation(xs, gif_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(open(gif_name,'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHi36Uy2Rm7r"
   },
   "source": [
    "## 2.2 Skipping Ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdPPaco1wLLO"
   },
   "source": [
    "We could take each image of our dataset and add noise to them `T` times to create `T` more new images, but do we need to? \n",
    "\n",
    "Thanks to the power of recursion, we can estimate what $x_t$ would look like given our beta schedule $\\beta_t$. A full breakdown of the math can be found in [Lilian Weng's Blog](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#speed-up-diffusion-model-sampling).\n",
    "Let's bring back `a`lpha, which is the compliment of $\\beta$. We can define $\\alpha_t$ as $1 - \\beta_t$, and we can define $\\bar{\\alpha}_t$ as the [cumulative product](https://pytorch.org/docs/stable/generated/torch.cumprod.html) of $\\alpha_t$.\n",
    "\n",
    "For example, $\\bar{\\alpha}_3 = \\alpha_0 \\cdot \\alpha_1 \\cdot \\alpha_2 \\cdot \\alpha_3$\n",
    "\n",
    "Because of the bar symbol, let's call $\\bar{\\alpha}_t$ `a_bar`. Our new noisy image distribution becomes:\n",
    "\n",
    "$q(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{\\bar{\\alpha}_{t}} \\cdot x_{0},(1 - \\bar{\\alpha}_t) \\cdot \\mathbf{I})$\n",
    "\n",
    "Which translates to code as:\n",
    "\n",
    "`x_t = sqrt_a_bar_t * x_0 + sqrt_one_minus_a_bar_t * noise`\n",
    "\n",
    "We are now no longer dependent on $\\mathbf{x}_{t-1}$ and can estimate $\\mathbf{x}_t$ from $x_0$. Let's define these variables in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myGGt3V7_Y1O"
   },
   "outputs": [],
   "source": [
    "a = 1. - B\n",
    "a_bar = torch.cumprod(a, dim=0)\n",
    "sqrt_a_bar = torch.sqrt(a_bar)  # Mean Coefficient\n",
    "sqrt_one_minus_a_bar = torch.sqrt(1 - a_bar) # St. Dev. Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GILry2GTF36w"
   },
   "source": [
    "We have all the pieces, let's code our forward diffusion sampling function `q`:\n",
    "\n",
    "$q(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{\\bar{\\alpha}_{t}} \\cdot \\mathbf{x}_{0},(1 - \\bar{\\alpha}_t) \\cdot \\mathbf{I})$\n",
    "\n",
    "Currently, `sqrt_a_bar` and `sqrt_one_minus_a_bar` only have one dimension, and if we index into them with `t`, they will each only have one value. If we want to multiply this value with each of the pixel values in our images, we will need to match the number of dimensions in order to [broadcast](https://numpy.org/doc/stable/user/basics.broadcasting.html).\n",
    "\n",
    "<center><img src=\"images/broadcasting.png\" width=\"60%\" /></center>\n",
    "\n",
    "We can add an extra dimension by indexing with `None`. This is a PyTorch shortcut to add an extra dimension to the resulting tensor. For reference, a batch of images has the dimensions: `batch dimension x image channels x image height x image width`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ih6Hcg9_FJLZ"
   },
   "outputs": [],
   "source": [
    "def q(x_0, t):\n",
    "    \"\"\"\n",
    "    Samples a new image from q\n",
    "    Returns the noise applied to an image at timestep t\n",
    "    x_0: the original image\n",
    "    t: timestep\n",
    "    \"\"\"\n",
    "    t = t.int()\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_a_bar_t = sqrt_a_bar[t, None, None, None]\n",
    "    sqrt_one_minus_a_bar_t = sqrt_one_minus_a_bar[t, None, None, None]\n",
    "\n",
    "    x_t = sqrt_a_bar_t * x_0 + sqrt_one_minus_a_bar_t * noise\n",
    "    return x_t, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyP6MdSfGStZ"
   },
   "source": [
    "Let's test out this new method compared to our old method of recursively generating the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "executionInfo": {
     "elapsed": 20057,
     "status": "ok",
     "timestamp": 1690448429069,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "FJU4Fy8JG5f6",
    "outputId": "8e355b67-f974-45dc-d8dd-137763084d3b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "xs = []\n",
    "\n",
    "for t in range(T):\n",
    "    t_tenser = torch.Tensor([t]).type(torch.int64)\n",
    "    x_t, _ = q(x_0, t_tenser)\n",
    "    img = torch.squeeze(x_t).cpu()\n",
    "    xs.append(img)\n",
    "    ax = plt.subplot(nrows, ncols, t + 1)\n",
    "    ax.axis('off')\n",
    "    other_utils.show_tensor_image(x_t)\n",
    "plt.savefig(\"forward_diffusion_skip.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 910,
     "output_embedded_package_id": "1XMyoAR8iugnUieuJIBQyWG8a5ClNQJG_"
    },
    "executionInfo": {
     "elapsed": 21978,
     "status": "ok",
     "timestamp": 1690448451035,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "Okel8MIzJdNT",
    "outputId": "a8918c27-afa8-482a-c943-4d133dbe378a"
   },
   "outputs": [],
   "source": [
    "gif_name = \"forward_diffusion_skip.gif\"\n",
    "other_utils.save_animation(xs, gif_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(open(gif_name,'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIQxClokPHVS"
   },
   "source": [
    "Compared to the previous technique, can you see any differences? When noise is added sequentially, there is a smaller difference between the images of consecutive timesteps. Despite this, the neural network will do a good job separating the noise from the original image in the reverse diffusion process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILsgVFmycEEM"
   },
   "source": [
    "## 2.3 Predicting Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRBp3ogo6SFn"
   },
   "source": [
    "The architecture for our neural network will mostly be the same as before. However, because the amount of noise added changes with each time step, we will need a way to tell the model which time step our input image is at.\n",
    "\n",
    "To do that, we can create an embedding block like the one below.\n",
    "* `input_dim` is the number of dimensions of the value we'd like to embed. We'll be embedding `t`, which is a one-dimensional scalar.\n",
    "* `emb_dim` is the number of dimensions we would like to convert our input value into by using a [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer.\n",
    "* [UnFlatten](https://pytorch.org/docs/stable/generated/torch.nn.Unflatten.html) is used to reshape a vector into a multidimensional space. Since we'll be adding the result of this embedding to a multidimensional feature map, we will add a few extra dimensions similar to how we expanded the dimension in the `q` function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjyvTEgIKmDr"
   },
   "outputs": [],
   "source": [
    "class EmbedBlock(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.Unflatten(1, (emb_dim, 1, 1))\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, self.input_dim)\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b_SGxJ-63sv"
   },
   "source": [
    "We will add this time embedding block to each `UpBlock` of our U-Net, resulting in the following architecture.\n",
    "\n",
    "<center><img src=\"images/time_nn.png\" width=\"80%\" /></center>\n",
    "\n",
    "**TODO**: Our `DownBlock` is the same as before. Using the above image as a reference, can you replace the `FIXME`s with the correct variable? Each `FIXME` can be one of:\n",
    "* `in_chs`\n",
    "* `out_chs`\n",
    "* `kernel_size`\n",
    "* `stride`\n",
    "* `padding`\n",
    "\n",
    "Click the `...` below for the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-ACwMgFKYlB"
   },
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs):\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(FIXME, FIXME, FIXME, FIXME, FIXME),\n",
    "            nn.BatchNorm2d(FIXME),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(FIXME, FIXME, FIXME, FIXME, FIXME),\n",
    "            nn.BatchNorm2d(FIXME),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs):\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_chs, out_chs, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrDOcqlbQArT"
   },
   "source": [
    "The `UpBlock` follows a similar logic, but instead uses [Transposed Convolution](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html).\n",
    "\n",
    "**TODO**: Can you replace the `FIXME`s with the correct variable? Each `FIXME` can be one of:\n",
    "* `in_chs`\n",
    "* `out_chs`\n",
    "* `kernel_size`\n",
    "* `stride`\n",
    "* `padding`\n",
    "* `strideT`\n",
    "* `out_paddingT`\n",
    "* `x`\n",
    "* `skip`\n",
    "\n",
    "Click the `...` below for the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJFm-gnaKZY-"
   },
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs):\n",
    "        # Convolution variables\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        # Transpose variables\n",
    "        strideT = 2\n",
    "        out_paddingT = 1\n",
    "\n",
    "        super().__init__()\n",
    "        # 2 * in_chs for concatenated skip connection\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(FIXME, FIXME, FIXME, FIXME, FIXME, FIXME),\n",
    "            nn.BatchNorm2d(FIXME),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(FIXME, FIXME, FIXME, FIXME, FIXME),\n",
    "            nn.BatchNorm2d(FIXME),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((FIXME, FIXME), 1)\n",
    "        x = self.model(FIXME)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs):\n",
    "        # Convolution variables\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        # Transpose variables\n",
    "        strideT = 2\n",
    "        out_paddingT = 1\n",
    "\n",
    "        super().__init__()\n",
    "        # 2 * in_chs for concatenated skip connection\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(2 * in_chs, out_chs, kernel_size, strideT, padding, out_paddingT),\n",
    "            nn.BatchNorm2d(out_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_chs, out_chs, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_chs),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2m56jgvvU_OB"
   },
   "source": [
    "The final U-Net is similar to what we used in the first lab. The difference is we now have a time embedding connected to our `UpBlock`s.\n",
    "\n",
    "**TODO**: While the time embeddings have been integrated into the model, there are still a number of `FIXME`s to replace. This time, the image channels, up channels, and down channels need fixing. Can you work down and up the U-Net to set the correct number of channels at each step?\n",
    "\n",
    "Each `FIXME` could be:\n",
    "* `img_chs`\n",
    "* A value in `down_chs`\n",
    "* A value in `up_chs`\n",
    "\n",
    "Click the `...` below for the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        img_chs = IMG_CH\n",
    "        down_chs = (16, 32, 64)\n",
    "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
    "        latent_image_size = IMG_SIZE // 4 # 2 ** (len(down_chs) - 1)\n",
    "        t_dim = 1 # New\n",
    "\n",
    "        # Inital convolution\n",
    "        self.down0 = nn.Sequential(\n",
    "            nn.Conv2d(FIXME, down_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(FIXME),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Downsample\n",
    "        self.down1 = DownBlock(down_chs[0], down_chs[1])\n",
    "        self.down2 = DownBlock(FIXME, FIXME)\n",
    "        self.to_vec = nn.Sequential(nn.Flatten(), nn.ReLU())\n",
    "        \n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Linear(FIXME*latent_image_size**2, down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], FIXME),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[2]*latent_image_size**2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.temb_1 = EmbedBlock(t_dim, up_chs[0]) # New\n",
    "        self.temb_2 = EmbedBlock(t_dim, up_chs[1]) # New\n",
    "        \n",
    "        # Upsample\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.Unflatten(1, (FIXME, latent_image_size, latent_image_size)),\n",
    "            nn.Conv2d(FIXME, up_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(up_chs[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UpBlock(up_chs[0], up_chs[1])\n",
    "        self.up2 = UpBlock(FIXME, FIXME)\n",
    "\n",
    "        # Match output channels\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(FIXME, FIXME, 3, 1, 1),\n",
    "            nn.BatchNorm2d(up_chs[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(up_chs[-1], img_chs, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        down0 = self.down0(x)\n",
    "        down1 = self.down1(down0)\n",
    "        down2 = self.down2(down1)\n",
    "        latent_vec = self.to_vec(down2)\n",
    "        \n",
    "        latent_vec = self.dense_emb(latent_vec)\n",
    "        # New\n",
    "        t = t.float() / T  # Convert from [0, T] to [0, 1]\n",
    "        temb_1 = self.temb_1(t)\n",
    "        temb_2 = self.temb_2(t)\n",
    "\n",
    "        up0 = self.up0(latent_vec)\n",
    "        up1 = self.up1(up0+temb_1, down2)\n",
    "        up2 = self.up2(up1+temb_2, down1)\n",
    "        return self.out(up2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        img_chs = IMG_CH\n",
    "        down_chs = (16, 32, 64)\n",
    "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
    "        latent_image_size = IMG_SIZE // 4 # 2 ** (len(down_chs) - 1)\n",
    "        t_dim = 1 # New\n",
    "\n",
    "        # Inital convolution\n",
    "        self.down0 = nn.Sequential(\n",
    "            nn.Conv2d(img_chs, down_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(down_chs[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Downsample\n",
    "        self.down1 = DownBlock(down_chs[0], down_chs[1])\n",
    "        self.down2 = DownBlock(down_chs[1], down_chs[2])\n",
    "        self.to_vec = nn.Sequential(nn.Flatten(), nn.ReLU())\n",
    "        \n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Linear(down_chs[2]*latent_image_size**2, down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[2]*latent_image_size**2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.temb_1 = EmbedBlock(t_dim, up_chs[0])  # New\n",
    "        self.temb_2 = EmbedBlock(t_dim, up_chs[1])  # New\n",
    "        \n",
    "        # Upsample\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
    "            nn.Conv2d(up_chs[0], up_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(up_chs[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UpBlock(up_chs[0], up_chs[1])\n",
    "        self.up2 = UpBlock(up_chs[1], up_chs[2])\n",
    "\n",
    "        # Match output channels\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(up_chs[-1], up_chs[-1], 3, 1, 1),\n",
    "            nn.BatchNorm2d(up_chs[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(up_chs[-1], img_chs, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        down0 = self.down0(x)\n",
    "        down1 = self.down1(down0)\n",
    "        down2 = self.down2(down1)\n",
    "        latent_vec = self.to_vec(down2)\n",
    "        \n",
    "        # New\n",
    "        t = t.float() / T  # Convert from [0, T] to [0, 1]\n",
    "        latent_vec = self.dense_emb(latent_vec)\n",
    "        temb_1 = self.temb_1(t)\n",
    "        temb_2 = self.temb_2(t)\n",
    "\n",
    "        up0 = self.up0(latent_vec)\n",
    "        up1 = self.up1(up0+temb_1, down2)\n",
    "        up2 = self.up2(up1+temb_2, down1)\n",
    "        return self.out(up2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1690448451038,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "8buyYGqLOiNP",
    "outputId": "ef8537b0-b494-4570-ee17-3d8738c2f73c"
   },
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model = torch.compile(UNet().to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw5Pw1VfV2m7"
   },
   "source": [
    "### 2.3.1 The Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYNL_m_tV5yn"
   },
   "source": [
    "In the first notebook, we used a [Mean Squared Error](https://developers.google.com/machine-learning/glossary#mean-squared-error-mse) loss function comparing the original image and the predicted original image based on noise.\n",
    "\n",
    "This time, we'll compare the real noise that was added to the image and the predicted noise. Lilian Weng goes into the math in this [blog post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#parameterization-of-l_t-for-training-loss). Originally, the loss function was based on the [Evidence Lower Bound (ELBO)](https://en.wikipedia.org/wiki/Evidence_lower_bound) [Log-Likelihood](https://mathworld.wolfram.com/Log-LikelihoodFunction.html), but it was found in the [Denoising Diffusion Probabilistic Models Paper](https://arxiv.org/abs/2006.11239) that the Mean Squared Error between the predicted noise and true noise was better in practice. If curious, Lilian Weng walks through the derivation [here](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#reverse-diffusion-process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxwt6WrnRqF4"
   },
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t):\n",
    "    x_noisy, noise = q(x_0, t)\n",
    "    noise_pred = model(x_noisy, t)\n",
    "    return F.mse_loss(noise, noise_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Reverse Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a model that predicts the noise added to an image at timestep `t`, but generating images is not as easy as repeatedly subtracting and adding noise. The `q` function can be reversed such that we generate $\\mathbf{x}_(t-1)$ from $\\mathbf{x}_t$.\n",
    "\n",
    "$q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_{t-1};{\\mathbf{\\tilde{\\mu}}}(\\mathbf{x_t},\\mathbf{x_0}), \\tilde{\\beta}_t \\cdot \\mathbf{I})$\n",
    "\n",
    "**Note**: $\\tilde{\\beta}_t$ was originally calculated to be $\\frac{1-\\overline{a}_{t-1}}{1-\\overline{a}_{t}}\\beta_t$, but in practice, using only $\\beta_t$ is more effective.\n",
    "\n",
    "Using [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem), we can derive an equation for the model_mean `u_t` at timestep `t`.\n",
    "\n",
    "${\\mathbf{\\tilde{\\mu}}}_t = \\frac{1}{\\sqrt{\\alpha_t}}(\\mathbf{x_t}-\\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha_t}}}\\mathbf{\\epsilon}_t)$\n",
    "\n",
    "The image $\\mathbf{x}_{t-1}$ can be estimated by ${\\mathbf{\\tilde{\\mu}}}_t + \\tilde{\\beta}_t \\cdot \\mathbf{I}$, so we'll use this equation to generate sample images recursively until we reach `t == 0`. Let's see what this means in code. First, we will precompute the values needed to calculate `u_t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_a_inv = torch.sqrt(1 / a)\n",
    "pred_noise_coeff = (1 - a) / torch.sqrt(1 - a_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create the reverse diffusion function, `reverse_q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4Wt6P13St5W"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def reverse_q(x_t, t, e_t):\n",
    "    t = torch.squeeze(t[0].int())  # All t values should be the same\n",
    "    pred_noise_coeff_t = pred_noise_coeff[t]\n",
    "    sqrt_a_inv_t = sqrt_a_inv[t]\n",
    "    u_t = sqrt_a_inv_t * (x_t - pred_noise_coeff_t * e_t)\n",
    "    if t == 0:\n",
    "        return u_t  # Reverse diffusion complete!\n",
    "    else:\n",
    "        B_t = B[t-1]\n",
    "        new_noise = torch.randn_like(x_t)\n",
    "        return u_t + torch.sqrt(B_t) * new_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to iteratively remove noise from an image until it is noise free. Let's also display these images so we can see how the model is improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJopDkbQR60S"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_images(ncols, figsize=(8,8)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    hidden_rows = T / ncols\n",
    "\n",
    "    # Noise to generate images from\n",
    "    x_t = torch.randn((1, IMG_CH, IMG_SIZE, IMG_SIZE), device=device)\n",
    "\n",
    "    # Go from T to 0 removing and adding noise until t = 0\n",
    "    plot_number = 1\n",
    "    for i in range(0, T)[::-1]:\n",
    "        t = torch.full((1,), i, device=device)\n",
    "        e_t = model(x_t, t)  # Predicted noise\n",
    "        x_t = reverse_q(x_t, t, e_t)\n",
    "        if i % hidden_rows == 0:\n",
    "            ax = plt.subplot(1, ncols+1, plot_number)\n",
    "            ax.axis('off')\n",
    "            other_utils.show_tensor_image(x_t.detach().cpu())\n",
    "            plot_number += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model! How about it? Does it look like the model is learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 92063,
     "status": "ok",
     "timestamp": 1690448543091,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "BncOxHBrTDly",
    "outputId": "160b838d-afb8-45fc-f7c2-fb2232bbdb62"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 3\n",
    "ncols = 15  # Should evenly divide T\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device)\n",
    "        x = batch[0].to(device)\n",
    "        loss = get_loss(model, x, t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1 == 0 and step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step:03d} | Loss: {loss.item()} \")\n",
    "            sample_images(ncols)\n",
    "print(\"Final sample:\")\n",
    "sample_images(ncols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vq_hWtK2cI2K"
   },
   "source": [
    "If you squint your eyes, can you make out what the model is generating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7384,
     "status": "ok",
     "timestamp": 1690448550463,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "Z-zupBmzcUuH",
    "outputId": "6602d55c-20fe-408d-8f43-d17c98a119d0"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "figsize=(8,8) # Change me\n",
    "ncols = 3 # Should evenly divide T\n",
    "for _ in range(10):\n",
    "    sample_images(ncols, figsize=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is learning ... something. It looks a little pixelated. Why would that be? Continue to the next notebook to find out more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMPRcXdvypGa4ncx029KLSM",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
